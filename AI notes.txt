Non-Convolutional-Model:

Run #1:
	Settings:
	Epochs = 50
	Learning Rate = 0.01
	Validation Split = 0.2
	Batch Size = 256

	Notes:
	The First epoch is the slowest, 2ms/step whilst the rest are 1ms/step.
	Loss decreases pretty rapidly in the beginning. From 1.8717 to 1.1499 to 0.7855 within only three (3) epochs.
	Accuracy also increases pretty rapidly. 0.4001 to 0.7349 to 0.8248 within three epochs.
	Val_loss: 1.4338 -> 0.8800 -> 0.6505
	Val_accuracy: 0.6294 -> 0.8172 -> 0.8547

	In 50 epochs the non-conv AI manages to reach 0.2396 loss and 0.9314 accuracy.

Run #2:
	Settings:
	Epochs = 50
	Learning Rate = 0.1
	Validation Split = 0.2
	Batch Size = 256

	Each step took about 800us (microseconds). The training model stopped at around 0.98 accuracy and 0.05 loss, while the 	validation model stopped around 0.965 accuracy and 0.125 loss.

Run #3:
	Settings:
	Epochs = 50
	Learning Rate = 1
	Validation Split = 0.2
	Batch Size = 256

	Each step took about 800us. The validation model gets really uneven in both accuracy and loss values.


Run #4:
	Settings:
	Epochs = 50
	Learning Rate = 0.05
	Validation Split = 0.2
	Batch Size = 256

	The training and validation model is really even but instead the accuracy and loss value is much worse. They stop at 	0.9669/0.9611 (train/validation) accuracy & 0.1171/0.1394 loss.

Run #5:
	Settings:
	Epochs = 50
	Learning Rate = 0.05
	Validation Split = 0.2
	Batch Size = 256

	At two times the amount of neurons (64) the model achieved 0.9729/0.9658 (train/valid) accuracy and 0.09774/0.1221 loss.
	The times between each epoch was around 900us/step, so a bit slower than with less neurons.


Run #6:
	Settings:
	Epochs = 50
	Learning Rate = 0.05
	Validation Split = 0.2
	Batch Size = 256

	At two times the amount of neurons (128) the model achieved 0.9769/0.9685 (train/valid) accuracy and 0.08346/0.1103 loss.
	The times between each epoch was around 960us/step, so a bit slower than with less neurons.



Convolutional Model:

Run #1:
	Settings:
	Epochs = 50
	Learning Rate = 0.01
	Validation Split = 0.2
	Batch Size = 256
	Kernel Size = (8, 8)

	Notes:
	First epoch had 1.4957 loss and 0.6237 accuracy.
	Every epoch took about 5 seconds and varied between 28-29ms/step. The first epoch took the longest at 6 seconds.
	The accuracy went to 0.9 pretty fast in only 7 epochs.
	In 50 epochs the conv AI managed to hit a loss of 0.1093 and an accuracy of 0.9691.
	This AI model is definitely much slower so far with these settings.

Run #2:
	Settings:
	Epochs = 50
	Learning Rate = 1
	Validation Split = 0.2
	Batch Size = 256
	Kernel Size = (8, 8)

	At 1.00 Learning Rate, accuracy and loss does not change and is at a stable 0.1 and 2.4 in value.

Run #3:
	Settings:
	Epochs = 50
	Learning Rate = 0.5
	Validation Split = 0.5
	Batch Size = 256
	Kernel Size = (8, 8)

	At 0.5 Learning Rate, the validation model is very uneven and differs in accuracy & loss value a lot. The training model is 		more stable but still worse than 0.01 LR.

Run #4:
	Settings:
	Epochs = 50
	Learning Rate = 0.2
	Validation Split = 0.2
	Batch Size = 256
	Kernel Size = (8, 8)

	At 0.2 Learning Rate, the validation and training model is much more even than at the previous 0.5 LR. The accuracy stops at 		around 0.985 and the loss rate stops at around 0.02.

Run #5:
	Settings:
	Epochs = 50
	Learning Rate = 0.11
	Validation Split = 0.2
	Batch Size = 256
	Kernel Size = (8, 8)

	At 0.11 LR, the training accuracy is even and stops at around 0.9946 and it's loss value at 0.02. The validation model has 	more uneven values in both accuracy and loss. It stops at around 0.985 accuracy and 0.055 loss.

Run #6:
	Settings:
	Epochs = 50
	Learning Rate = 0.1
	Validation Split = 0.2
	Batch Size = 128
	Kernel Size = (8, 8)

	At batch size 128, each steps take half the amount of time than with 256 in batch size. The training model is even and stops 	at around 0.9972 accuracy and 0.01247 loss. The validation stops at 0.985 accuracy and 0.049 loss.


Run #7:
	Settings:
	Epochs = 50
	Learning Rate = 0.1
	Validation Split = 0.2
	Batch Size = 64
	Kernel Size = (8, 8)

	At batch size 64, each steps take half the time than the previous run at 128 batch size. The training model stops at 0.9992 		accuracy and 0.004 loss while validation stops at 0.9858 & 0.05482 loss.

Run #8:
	Settings:
	Epochs = 50
	Learning Rate = 0.1
	Validation Split = 0.2
	Batch Size = 256
	Kernel Size = (8, 8)

	With four times more neurons (64) than before the model achieved 0.9911/0.9854 (train/valid) accuracy and 0.03172/0.04813 		loss. It took way longer to run through all the epochs. 

Run #9:
	Settings:
	Epochs = 50
	Learning Rate = 0.1
	Validation Split = 0.2
	Batch Size = 256
	Kernel Size = (8, 8)

	With 128 neurons the model achieved 0.09916/0.9857 (train/valid) accuracy and 0.03089/0.04789 loss. It took almost 8 minutes 	to finish all the epochs. Around 53ms/step.


c. Det vanliga neuronnätet (non-convolutional) gick igenom varje epok mycket snabbare än CNN. 
Ett Non-convolutional neural network går igenom en bild varje pixel för sig, medan ett convolutional NN går igenom en bild med större block som innehåller sub-NNs.

d. Under första körningen vinner CNN i jämförelse med ANN då efter 50 epoker hade den en bättre accuracy med mycket mindre loss. Dock tar CNN mycket längre tid att köra igenom alla 50 epoker i jämförelse med ANN.

e.