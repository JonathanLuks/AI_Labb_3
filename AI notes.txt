Non-Convolutional-Model:

Run #1:
	Settings:
	Epochs = 50
	Learning Rate = 0.01
	Validation Split = 0.2
	Batch Size = 256

	Notes:
	The First epoch is the slowest, 2ms/step whilst the rest are 1ms/step.
	Loss decreases pretty rapidly in the beginning. From 1.8717 to 1.1499 to 0.7855 within only three (3) epochs.
	Accuracy also increases pretty rapidly. 0.4001 to 0.7349 to 0.8248 within three epochs.
	Val_loss: 1.4338 -> 0.8800 -> 0.6505
	Val_accuracy: 0.6294 -> 0.8172 -> 0.8547

	In 50 epochs the non-conv AI manages to reach 0.2396 loss and 0.9314 accuracy.

Run #2:
	Settings:
	Epochs = 50
	Learning Rate = 0.1
	Validation Split = 0.2
	Batch Size = 256

	Each step took about 800us (microseconds). The training model stopped at around 0.98 accuracy and 0.05 loss, while the 		validation model stopped around 0.965 accuracy and 0.125 loss.

Run #3:
	Settings:
	Epochs = 50
	Learning Rate = 1
	Validation Split = 0.2
	Batch Size = 256

	Each step took about 800us. The validation model gets really uneven in both accuracy and loss values.


Run #4:
	Settings:
	Epochs = 50
	Learning Rate = 0.05
	Validation Split = 0.2
	Batch Size = 256

	The training and validation model is really even but instead the accuracy and loss value is much worse. They stop at 		0.9669/0.9611 (train/validation) accuracy & 0.1171/0.1394 loss.

Run #5:
	Settings:
	Epochs = 50
	Learning Rate = 0.05
	Validation Split = 0.2
	Batch Size = 256

	At two times the amount of neurons (64) the model achieved 0.9729/0.9658 (train/valid) accuracy and 0.09774/0.1221 loss.
	The times between each epoch was around 900us/step, so a bit slower than with less neurons.


Run #6:
	Settings:
	Epochs = 50
	Learning Rate = 0.05
	Validation Split = 0.2
	Batch Size = 256

	At two times the amount of neurons (128) the model achieved 0.9769/0.9685 (train/valid) accuracy and 0.08346/0.1103 loss.
	The times between each epoch was around 960us/step, so a bit slower than with less neurons.



Convolutional Model:

Run #1:
	Settings:
	Epochs = 50
	Learning Rate = 0.01
	Validation Split = 0.2
	Batch Size = 256
	Kernel Size = (8, 8)

	Notes:
	First epoch had 1.4957 loss and 0.6237 accuracy.
	Every epoch took about 5 seconds and varied between 28-29ms/step. The first epoch took the longest at 6 seconds.
	The accuracy went to 0.9 pretty fast in only 7 epochs.
	In 50 epochs the conv AI managed to hit a loss of 0.1093 and an accuracy of 0.9691.
	This AI model is definitely much slower so far with these settings.

Run #2:
	Settings:
	Epochs = 50
	Learning Rate = 1
	Validation Split = 0.2
	Batch Size = 256
	Kernel Size = (8, 8)

	At 1.00 Learning Rate, accuracy and loss does not change and is at a stable 0.1 and 2.4 in value.

Run #3:
	Settings:
	Epochs = 50
	Learning Rate = 0.5
	Validation Split = 0.5
	Batch Size = 256
	Kernel Size = (8, 8)

	At 0.5 Learning Rate, the validation model is very uneven and differs in accuracy & loss value a lot. The training model is 		more stable but still worse than 0.01 LR.

Run #4:
	Settings:
	Epochs = 50
	Learning Rate = 0.2
	Validation Split = 0.2
	Batch Size = 256
	Kernel Size = (8, 8)

	At 0.2 Learning Rate, the validation and training model is much more even than at the previous 0.5 LR. The accuracy stops at 		around 0.985 and the loss rate stops at around 0.02.

Run #5:
	Settings:
	Epochs = 50
	Learning Rate = 0.11
	Validation Split = 0.2
	Batch Size = 256
	Kernel Size = (8, 8)

	At 0.11 LR, the training accuracy is even and stops at around 0.9946 and it's loss value at 0.02. The validation model has 		more uneven values in both accuracy and loss. It stops at around 0.985 accuracy and 0.055 loss.

Run #6:
	Settings:
	Epochs = 50
	Learning Rate = 0.1
	Validation Split = 0.2
	Batch Size = 128
	Kernel Size = (8, 8)

	At batch size 128, each steps take half the amount of time than with 256 in batch size. The training model is even and stops 		at around 0.9972 accuracy and 0.01247 loss. The validation stops at 0.985 accuracy and 0.049 loss.


Run #7:
	Settings:
	Epochs = 50
	Learning Rate = 0.1
	Validation Split = 0.2
	Batch Size = 64
	Kernel Size = (8, 8)

	At batch size 64, each steps take half the time than the previous run at 128 batch size. The training model stops at 0.9992 		accuracy and 0.004 loss while validation stops at 0.9858 & 0.05482 loss.

Run #8:
	Settings:
	Epochs = 50
	Learning Rate = 0.1
	Validation Split = 0.2
	Batch Size = 256
	Kernel Size = (8, 8)

	With four times more neurons (64) than before the model achieved 0.9911/0.9854 (train/valid) accuracy and 0.03172/0.04813 		loss. It took way longer to run through all the epochs. 

Run #9:
	Settings:
	Epochs = 50
	Learning Rate = 0.1
	Validation Split = 0.2
	Batch Size = 256
	Kernel Size = (8, 8)

	With 128 neurons the model achieved 0.09916/0.9857 (train/valid) accuracy and 0.03089/0.04789 loss. It took almost 8 minutes 		to finish all the epochs. Around 53ms/step.

Run #10:
	Settings:
	Epochs = 50
	Learning Rate = 0.1
	Validation Split = 0.2
	Batch Size = 256
	Kernel Size = (16, 16)
	Strides = (2, 2)

	Doubling the Kernel Size and Strides the model achieved 0.9815/0.9765 accuracy in training and validation
	and 0.06418/0.08072 loss. The average accuracy is worse with these values.

Run #11:
	Settings:
	Epochs = 50
	Learning Rate = 0.1
	Validation Split = 0.2
	Batch Size = 256
	Kernel Size = (12, 12)
	Strides = (2, 2)

	Lowering the Kernel Size a tiny bit the model reached a little better accuracy and loss 
	at around 0.9845/0.9793 & 0.05364/0.0684. The moved data average accuracy was at 20.88, a little better than the previous 		model.

Run #12:
	Settings:
	Epochs = 50
	Learning Rate = 0.1
	Validation Split = 0.2
	Batch Size = 256
	Kernel Size = (12, 12)
	Strides = (1, 1)

	This model reached 0.9898/0.9857 accuracy & 0.03578/0.05074 loss. Moved data: 21.24 avg accuracy. Rotated data: 89.03 avg.

Run #13:
	Settings:
	Epochs = 50
	Learning Rate = 0.1
	Validation Split = 0.2
	Batch Size = 256
	Kernel Size = (20, 20)
	Strides = (1, 1)

	0.9836/0.9778 accuracy & 0.0577/0.07254 loss. Moved data avg accuracy: 18.58. Rotated: 85.85 avg.

Run #14:
	Settings:
	Epochs = 50
	Learning Rate = 0.1
	Validation Split = 0.2
	Batch Size = 256
	Kernel Size = (15, 15)
	Strides = (1, 1)

	0.9896/0.9835 accuracy & 0.03717/0.05467 loss. Moved data avg accuracy: 20.12. Rotated: 89.55 avg.

Run #15:
	Settings:
	Epochs = 50
	Learning Rate = 0.1
	Validation Split = 0.2
	Batch Size = 256
	Kernel Size = (14, 14)
	Strides = (1, 1)

	Exactly the same as the previous one.

Run #16:
	Settings:
	Epochs = 50
	Learning Rate = 0.1
	Validation Split = 0.2
	Batch Size = 256
	Kernel Size = (10, 10)
	Strides = (1, 1)

	Almost identical to Run #12.

Run #17:
	Settings:
	Epochs = 50
	Learning Rate = 0.1
	Validation Split = 0.2
	Batch Size = 256
	Kernel Size = (10, 10)
	Strides = (2, 2)

	0.9861/0.9815 accuracy. 0.0472/0.06246 loss. Moved data: 21.48 avg. Rotated: 88.2 avg.

Run #18:
	Settings:
	Epochs = 50
	Learning Rate = 0.1
	Validation Split = 0.2
	Batch Size = 256
	Kernel Size = (20, 20)
	Strides = (2, 2)

	Garbage.

Run #19:
	Settings:
	Epochs = 50
	Learning Rate = 0.1
	Validation Split = 0.2
	Batch Size = 256
	Kernel Size = (6, 6)
	Strides = (1, 1)

	Worse and slower.

Run #20:
	Settings:
	Epochs = 50
	Learning Rate = 0.1
	Validation Split = 0.2
	Batch Size = 256
	Kernel Size = (9, 9)
	Strides = (1, 1)

	0.9905/0.9847 accuracy & 0.0336/0.05163 loss. Moved data: 21.94 avg. Rotated: 89.86 avg

Run #21:
	Settings:
	Epochs = 50
	Learning Rate = 0.1
	Validation Split = 0.2
	Batch Size = 256
	Kernel Size = (9, 9)
	Strides = (1, 1)

	After adding a new layer to the model it achieved 0.9957/0.9841 accuracy and 0.01539/0.05914 loss. Moved: 21.3 avg. 
	Rotated: 91.11 avg. The model took nine minutes to train and validate. Each step took around 62ms.



c. Det vanliga neuronnätet (non-convolutional) gick igenom varje epok mycket snabbare än CNN. 
Ett Non-convolutional neural network går igenom en bild varje pixel för sig, medan ett convolutional NN går igenom en bild med större block som innehåller sub-NNs.

d. Under första körningen vinner CNN i jämförelse med ANN då efter 50 epoker hade den en bättre accuracy med mycket mindre loss. Dock tar CNN mycket längre tid att köra igenom alla 50 epoker i jämförelse med ANN.

e. Lär sig lite mer, men tar väldigt mycket mer tid. Inte värt att använda då modeller med mindre neuroner kan lära sig ungefär lika mycket på mycket kortare tid.

f. Att öka Kernel Size över (10, 10) försämrar modellen. Att öka Strides försämrar också modellen. 

g. Precisionen blir mycket bättre men detta i kostnad mot träningstid, som även ökar väldigt mycket.





8. Egna experiment:

Conv:
Test #1:
	Settings:
	Epochs = 50
	Learning Rate = 0.3
	Batch Size = 64
	Kernel Size = (8, 8)
	Strides = (1, 1)
	Layers = 2
	Neurons = 32


	1/0.9897 Accuracy (train/validation). 2.1445*10^-5/0.007792 (train/validation) Loss. Moved: 23.74 avg. Rotated: 89.92 avg.
	21 minutes 47 seconds elapsed.


Test #2:
	Settings:
	Epochs = 50
	Learning Rate = 0.35
	Batch Size = 64
	Kernel Size = (8, 8)
	Strides = (1, 1)
	Layers = 1
	Neurons = 16


	0.9963/0.9794 Accuracy (train/validation). 0.01093/0.1201 Loss (train/validation). Moved: 21.01 avg. Rotated: 86.47 avg.
	2 minutes 15 seconds elapsed.


Test #3:
	Settings:
	Epochs = 50
	Learning Rate = 0.4
	Batch Size = 64
	Kernel Size = (8, 8)
	Strides = (1, 1)
	Layers = 1
	Neurons = 32


	1/0.9875 Accuracy. 2.9956*10^-4/0.06705 Loss. Moved: 21.65 avg. Rotated: 90.17 avg.
	2 minutes 57 seconds elapsed.


Test #4:
	Settings:
	Epochs = 50
	Learning Rate = 0.4
	Batch Size = 32
	Kernel Size = (8, 8)
	Strides = (1, 1)
	Layers = 1
	Neurons = 64

	1/0.9866 Accuracy. 8.0342*10^-5/0.0895 Loss. Moved: 21.27 avg. Rotated: 89.29 avg.
	5m 22s elapsed.


Test #5:
	Settings:
	Epochs = 50
	Learning Rate = 0.35
	Batch Size = 128
	Kernel Size = (8, 8)
	Strides = (1, 1)
	Layers = 1
	Neurons = 32


	1/0.9877 Accuracy. 8.7093*10^-4/0.05637 Loss. Moved: 21.86 avg. Rotated: 89.97 avg.
	2m 59s elapsed.


Answer: Test #3 was the best!! around!!! nothing's gonna ever keep me down!!!



non-conv:
Test #1:
	Settings:
	Epochs = 50
	Learning Rate = 0.3
	Batch Size = 64´
	Neurons = 32


	0.994/0.9651 Accuracy. 0.01946/0.2458 Loss. Moved: 15.54 avg. Rotated: 82.28 avg.
	23s elapsed.


Test #2:
	Settings:
	Epochs = 50
	Learning Rate = 0.4
	Batch Size = 64
	Neurons = 64


	1/0.9769 Accuracy. 6.8722*10^-4/0.1196 Loss. Moved: 16.73 avg. Rotated: 85.98 avg.
	24s elapsed.


Test #3:
	Settings:
	Epochs = 50
	Learning Rate = 0.4
	Batch Size = 64
	Neurons = 128


	1/0.9789 Accuracy. 5.4733*10^-4/0.09306 Loss. Moved: 17.39 avg. Rotated: 87.68 avg.
	25s elapsed.


Test #4:
	Settings:
	Epochs = 50
	Learning Rate = 0.4
	Batch Size = 64
	Neurons = 1024


	1/0.9827 Accuracy. 4.2237*10^-4/0.07589 Loss. Moved: 17.51 avg. Rotated: 88.36 avg.
	1m 19s elapsed.

Test #5:
	Settings:
	Epochs = 50
	Learning Rate = 0.4
	Batch Size = 64
	Neurons = 16384


	1/0.9842 Accuracy. 4.05*10^-4/0.07187 Loss. Moved: 17.39 avg. Rotated: 88.42 avg.
	13m 47s elapsed.


Test #6:
	Settings:
	Epochs = 50
	Learning Rate = 0.4
	Batch Size = 32
	Neurons = 1024


	1/0.9836 Accuracy. 1.5577*10^-4/0.08045 Loss. Moved: 17.69 avg. Rotated: 89.04 avg.
	2m 36s elapsed.


Answer: Test #6 was best!! around!!! nothing's gonna ever keep me down!!

